{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 5. Support Vector Machines\n",
    "**SVMs are particularly well suited for classification of complex small-or medium-sized datasets**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear SVM Classification\n",
    "#### Large margin classification: \n",
    "* **The line not only separates the two classes but also stays fat away from the closest training instances as possible**\n",
    "* **The marigin is fully determined(or \"supported\") by the instances locates on the edge of the street,these instances are calles *support vectors***\n",
    "* **SVMs are sensitve to feature scales，needs to implement** StandardScaler **before applying the decision boundary**\n",
    "\n",
    "#### Hard Margin Classification:\n",
    "* **If we strictly impose that all instances must be off the street on the right side, this is called *hard margin calssification***\n",
    "* **Hard margin classification only works if the data is linearly sepearable**\n",
    "* **Hard margin classification is sensitive to outliers**\n",
    "\n",
    "#### Soft Margin Classification:\n",
    "* **The objective is to find a good balance between keeping the street as large as possible and limiting the *margin violations***\n",
    "* **When creating an SVM model using Scikit-Learn, we can specify a number of hyperparameters。** .C **is one of those hyperparameters**\n",
    "* **If your SVM model is overfitting, you can try regularizing it by reducing** C.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Loads the iris dataset, scales the features, and then trains a linear SVM model to detect *Iris virginica* flowers**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import LinearSVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris=datasets.load_iris()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=iris[\"data\"][:,(2,3)]#petal length, petal width"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "y=(iris[\"target\"]==2).astype(np.float64)#Iris virginica"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_clf=Pipeline([\n",
    "    (\"scaler\",StandardScaler()),\n",
    "    (\"linear_svc\",LinearSVC(C=1,loss=\"hinge\")),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('scaler', StandardScaler()),\n",
       "                ('linear_svc', LinearSVC(C=1, loss='hinge'))])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svm_clf.fit(X,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Use the model to make predictions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svm_clf.predict([[5.5,1.7]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Unlike Logistic Regression classifiers, SVM classifiers do not output probabilites for each class**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Similarity Features\n",
    "* **Another technique to tackle nonlinear problem is to add features computed using a similarity function, which measures how much each instance resembles a particular *landmark***\n",
    "* **Gausssian Radial Basis Function(RBF): Bell shaped function varying from 0(very far away from the landmark) to 1(at the landmark), resulting transformation will be linearly separable**\n",
    "* **How to select a landmark**\n",
    "    * **The simplest approach is to create a landamark at the location of each every instance in the dataset**\n",
    "    * **Doing that creates many dimensions and thus increases the chances that the trainsformed training set will be linearly separable**\n",
    "    * **If your training set is bery large, you end up with an equally large number of features**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gaussian RBF Kernel\n",
    "**The similarity features method can be useful with any Machine Learning algorithm, but it may be computationally expensive expensive to compute the additional features, especially on large training sets.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "rbf_kernel_svm_clf=Pipeline([\n",
    "    (\"scaler\",StandardScaler()),\n",
    "    (\"svm_clf\",SVC(kernel=\"rbf\",gamma=5,C=0.001))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('scaler', StandardScaler()),\n",
       "                ('svm_clf', SVC(C=0.001, gamma=5))])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rbf_kernel_svm_clf.fit(X,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Increasing** gamma **makes the bell-shaped curve narrower. As a result, each instance's range of influence is smaller: the decision boundary ends up being more irregular, waggling around individual instances**\n",
    "* **Conversely, a small** gamma **valye makes the bell-shaped curvewider: instances have a larger range of influence, and the decision boundary ends uo smoother**\n",
    "* gamma **acts like a regularization hyperparameter: if your model is overfitting, you should reduce it; if it is underfitting, you should increase it**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Selecting Kernels:\n",
    "* **You should always try the linear kernel first, especially if the training set is very large of if it has plenty of features**（LinearSVC **is much faster than** SVC(kernel=\"linear\")), **especially if the trainning set is very large or if they have plenty of features**\n",
    "* **If the trainning set is not too large, you should also try the Gaussian RBF kernel.**\n",
    "\n",
    "### Computational Complexity\n",
    "* LinearSVC **class is based ont the** liblinear **library, which implements an optimized algorithm fot linear SVM. The training time complexity is roughly** O(m*n)\n",
    "     * **The algorithm takes longer if you require very high precision(Controlled by tolerance parameter e)\n",
    "* **The** SVC **class is based on the** libsvm **library, has a training time complexity between O(m^2*n) and O(m^3*n)**\n",
    "    * **It scaled well with the number of features, especailly with *sparse features***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM Regression\n",
    "* **Instead of classification, the trick is to reverse the objective: instead of trying to fit the largest possible street between two classes while limiting margin violations, SVM Regression tries to fit as many instances as possible on the street while limiting margin violations**\n",
    "* **The widith of the street is controlled by a hyperparameter, E.**\n",
    "* **Adding more training instances within the margin does not affect the model's predicitons; thus, the model is said to be *E-sensitive***\n",
    "* **You can use SK-Leanr's** LinearSVR **class to perform linear SVM Regression**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_reg=LinearSVR(epsilon=1.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearSVR(epsilon=1.5)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svm_reg.fit(X,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**To tackle nonlinear regression, you an use a kernelized SVM model.**\n",
    "* **The** SVR **class is the regression equivalent of the** SVC **class, and the** LinearSVR **class is the regression equivalent of the** LinearSVC **class**\n",
    "* **The** LinearSVR **class scales linearly with the size of the training set(just like the** LinearSVC **class), while the** SVR **class gets much too slow when the training set grows large**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_poly_reg=SVR(kernel=\"poly\",degree=2,C=100,epsilon=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVR(C=100, degree=2, kernel='poly')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svm_poly_reg.fit(X,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Under the Hood\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

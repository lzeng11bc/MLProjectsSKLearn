{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 7. Ensemble Learning and Random Forests\n",
    "* **If you aggregate your predictions of a group of predictors(such as classifiers or regressors), you will often get better predictions than with the best individual predictors**\n",
    "* **A group of predictors is called an *ensemble*, this technique is called *Ensemble Learning*, and an Ensemble Learning algorithm is called an *Ensemble method***\n",
    "* ***Random Forest:* You can train a group of Decision Tree classifiers, each on a different random subset of the trainning set.**\n",
    "* **You will often use Ensemble methods near the end of a project, once you have already build a few good predictors, to combine them into an even better predictor.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Voting Classifiers\n",
    "* ***hard voting* classifier: A very simple way to create an even better classifier is to aggregate the predictions of each classifier and predict the class that gets the most votes**\n",
    "* **This voting classifier often achieves a higher accuracy than the best classifier in the ensemble**\n",
    "* **This is only true if all classifiers are perfectly independent, making uncorrelated errors**\n",
    "* **Because most classifiers are trained on the same data, they are likely to make the same type of errors, so there will be many majority votes for the wrong classes, reducing the ensemble's accuracy**\n",
    "* **One way to get diverse classifiers is to train them using very different algorithms. They increase the chance that they will make very different type of errors, improving the ensemble's accuracy**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Creates and trains a voting classifier in SK-Learn, composed of three diverse classifiers**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<frozen importlib._bootstrap>:219: RuntimeWarning: numpy.ufunc size changed, may indicate binary incompatibility. Expected 192 from C header, got 216 from PyObject\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import make_moons\n",
    "\n",
    "X, y = make_moons(n_samples=500, noise=0.30, random_state=42)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_clf=LogisticRegression()\n",
    "rnd_clf=RandomForestClassifier()\n",
    "svm_clf=SVC()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "voting_clf=VotingClassifier(\n",
    "    estimators=[('lr',log_clf),('rf',rnd_clf),('svc',svm_clf)],\n",
    "    voting='hard'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VotingClassifier(estimators=[('lr', LogisticRegression()),\n",
       "                             ('rf', RandomForestClassifier()), ('svc', SVC())])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "voting_clf.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Look at each classifier's accuracy on the test set**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression 0.864\n",
      "RandomForestClassifier 0.896\n",
      "SVC 0.896\n",
      "VotingClassifier 0.912\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "for clf in (log_clf,rnd_clf,svm_clf,voting_clf):\n",
    "    clf.fit(X_train,y_train)\n",
    "    y_pred=clf.predict(X_test)\n",
    "    print(clf.__class__.__name__,accuracy_score(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***soft voting:* If all classifiers are able to estimate the class probabilities, then you can tell sk-learn to predict the class with the highest class probabilty, avergaed over all the individual classifiers**\n",
    "* **Often achieves higher performance than hard voting because it gives more weight to highly confident votes**\n",
    "* **All you need to do is replace** voting=\"hard\" **with** voting=\"soft\" **and ensure that all classifiers can estimate class porbabilities**\n",
    "* **Also you need to ensure that all classifiers have** predict_proba() **method**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bagging and Pasting\n",
    "* **Another apporach is to use the same trainning algorithm for every predictor and train them on different random subsets of the trainning sample**\n",
    "* **When sampling is performed *with* replacement, this method is called *bagging***\n",
    "* **When sampling is performed without replacement, it is called *pasting***\n",
    "* **Both bagging and pasting allow trainning instances to be sampled several times across multiple predictors, but only bagging allows trainning instances to be sampled several times for the same predictor.**\n",
    "* **Once predictors are trained, the ensemble can make predictos for a new instance by simply aggregating the predictions of all predictors**\n",
    "* **The aggregation is typically the *statistical mode* for classification, or the averge for regression**\n",
    "* **Generally, the net result is that the ensemble has a similar bias but a lower variance than a single predictor trained on the original training set**\n",
    "* **Predictors can all be trained in parallel, via different CPU cores or even different servers.**\n",
    "* **Similarly, predictions can be made in parallel: bagging and pasting scales very well**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bagging and Pasting in Scikit-Learn\n",
    "* **SK-Learn offers a simple API for both bagging and pasting with the** BaggingClassifier **class(or** BaggingRegressor **for regression).**\n",
    "* **In bagging, use** bootstrap=True. **if you wwant to use pasting, set** bootstrap=False\n",
    "* **The** n_jobs **parameter tells SK-Learn the number of CPU cores to use for training and predictions(-1 tells SK-Learn to use all availiable cores)**\n",
    "* **The** BaggingClassifier **automatically performs soft voting instead of hard voting if the base classifier can estimate class probabilities(i.e., if it has a** predict_proba( ) **method), which is the case with Decision Tree classifiers**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#contains an ensemble of 500 Decision Tree classifiers\n",
    "bag_clf=BaggingClassifier(\n",
    "    DecisionTreeClassifier(),n_estimators=500,\n",
    "    max_samples=100,bootstrap=True,n_jobs=-1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BaggingClassifier(base_estimator=DecisionTreeClassifier(), max_samples=100,\n",
       "                  n_estimators=500, n_jobs=-1)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bag_clf.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred=bag_clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.904\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "print(accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **The ensembe has a comparable bias but a smaller variance(it makes roughly the same number of errors on the trainning set, but the decision boundary is less irregular)**\n",
    "* **Bootstrapping introduces a bit more diversity in the subsets that each predictor is trained on, so bagging ends up with a slightly higher bias than pasting; but the extra diversityh also means that the predictors end up being less correlated, so the ensemble's variance is reduced**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Out-of-Bag Evaluation\n",
    "* **By default, a** BaggingClassifier **samples *m* training instances with replacement, where *m* is the size of the training set.**\n",
    "* **This means that only 63% of the trainning instances are sampled on average for each predictor**\n",
    "* **A bagging snsemble can be evaluated using oob instances, without the need for a separaye validation set**\n",
    "* **In SK-learn, you can set** oob_score=True **when creating a bagging classifier to request an automatica oob evaluation after trainning**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "bag_clf=BaggingClassifier(\n",
    "    DecisionTreeClassifier(),n_estimators=500,\n",
    "    bootstrap=True, n_jobs=-1,oob_score=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BaggingClassifier(base_estimator=DecisionTreeClassifier(), n_estimators=500,\n",
       "                  n_jobs=-1, oob_score=True)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bag_clf.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**According to this oob evaluation, this** BaggingClassifier **is likely to achieve about 90.1% accuracy on the test set**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.896"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bag_clf.oob_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred=bag_clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.912"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_test,y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The oob decision function for each training instance is also availiable through** oob_decision_function_ **variable. In this case(since the base estimator has a** predic_proba( ) **method), the decision function returns the class probabiliteis for each trainning instance**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.34391534, 0.65608466],\n",
       "       [0.32160804, 0.67839196],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.04918033, 0.95081967],\n",
       "       [0.33870968, 0.66129032],\n",
       "       [0.01176471, 0.98823529],\n",
       "       [0.99404762, 0.00595238],\n",
       "       [0.98324022, 0.01675978],\n",
       "       [0.74731183, 0.25268817],\n",
       "       [0.00546448, 0.99453552],\n",
       "       [0.785     , 0.215     ],\n",
       "       [0.84065934, 0.15934066],\n",
       "       [0.95321637, 0.04678363],\n",
       "       [0.05319149, 0.94680851],\n",
       "       [0.        , 1.        ],\n",
       "       [0.98536585, 0.01463415],\n",
       "       [0.95698925, 0.04301075],\n",
       "       [0.99494949, 0.00505051],\n",
       "       [0.04      , 0.96      ],\n",
       "       [0.37222222, 0.62777778],\n",
       "       [0.90322581, 0.09677419],\n",
       "       [1.        , 0.        ],\n",
       "       [0.96891192, 0.03108808],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.64044944, 0.35955056],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.14444444, 0.85555556],\n",
       "       [1.        , 0.        ],\n",
       "       [0.00617284, 0.99382716],\n",
       "       [0.31413613, 0.68586387],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.1744186 , 0.8255814 ],\n",
       "       [0.37142857, 0.62857143],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.02150538, 0.97849462],\n",
       "       [1.        , 0.        ],\n",
       "       [0.0273224 , 0.9726776 ],\n",
       "       [0.99404762, 0.00595238],\n",
       "       [0.88202247, 0.11797753],\n",
       "       [0.95054945, 0.04945055],\n",
       "       [0.94818653, 0.05181347],\n",
       "       [0.        , 1.        ],\n",
       "       [0.06122449, 0.93877551],\n",
       "       [0.98895028, 0.01104972],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.00518135, 0.99481865],\n",
       "       [0.98913043, 0.01086957],\n",
       "       [0.8       , 0.2       ],\n",
       "       [0.36702128, 0.63297872],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.73762376, 0.26237624],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.84971098, 0.15028902],\n",
       "       [1.        , 0.        ],\n",
       "       [0.6440678 , 0.3559322 ],\n",
       "       [0.06707317, 0.93292683],\n",
       "       [0.66666667, 0.33333333],\n",
       "       [0.87634409, 0.12365591],\n",
       "       [0.        , 1.        ],\n",
       "       [0.21052632, 0.78947368],\n",
       "       [0.8956044 , 0.1043956 ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.05181347, 0.94818653],\n",
       "       [0.03125   , 0.96875   ],\n",
       "       [0.37096774, 0.62903226],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.84916201, 0.15083799],\n",
       "       [0.00543478, 0.99456522],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.25      , 0.75      ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.92696629, 0.07303371],\n",
       "       [0.75879397, 0.24120603],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.21052632, 0.78947368],\n",
       "       [0.62359551, 0.37640449],\n",
       "       [0.        , 1.        ],\n",
       "       [0.03125   , 0.96875   ],\n",
       "       [0.50793651, 0.49206349],\n",
       "       [0.99497487, 0.00502513],\n",
       "       [0.0257732 , 0.9742268 ],\n",
       "       [0.99450549, 0.00549451],\n",
       "       [0.25301205, 0.74698795],\n",
       "       [0.48235294, 0.51764706],\n",
       "       [1.        , 0.        ],\n",
       "       [0.01657459, 0.98342541],\n",
       "       [1.        , 0.        ],\n",
       "       [0.29292929, 0.70707071],\n",
       "       [0.93617021, 0.06382979],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.81463415, 0.18536585],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.98453608, 0.01546392],\n",
       "       [1.        , 0.        ],\n",
       "       [0.00555556, 0.99444444],\n",
       "       [0.96174863, 0.03825137],\n",
       "       [1.        , 0.        ],\n",
       "       [0.01169591, 0.98830409],\n",
       "       [0.17708333, 0.82291667],\n",
       "       [0.95811518, 0.04188482],\n",
       "       [0.2970297 , 0.7029703 ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.00534759, 0.99465241],\n",
       "       [0.74594595, 0.25405405],\n",
       "       [0.41477273, 0.58522727],\n",
       "       [0.40782123, 0.59217877],\n",
       "       [0.86486486, 0.13513514],\n",
       "       [0.94818653, 0.05181347],\n",
       "       [0.06626506, 0.93373494],\n",
       "       [0.77173913, 0.22826087],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.00980392, 0.99019608],\n",
       "       [0.97109827, 0.02890173],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.01666667, 0.98333333],\n",
       "       [0.        , 1.        ],\n",
       "       [0.01010101, 0.98989899],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.94623656, 0.05376344],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.98895028, 0.01104972],\n",
       "       [0.        , 1.        ],\n",
       "       [0.36458333, 0.63541667],\n",
       "       [0.25728155, 0.74271845],\n",
       "       [0.00510204, 0.99489796],\n",
       "       [0.        , 1.        ],\n",
       "       [0.3220339 , 0.6779661 ],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.97326203, 0.02673797],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.00502513, 0.99497487],\n",
       "       [0.62564103, 0.37435897],\n",
       "       [0.94736842, 0.05263158],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.07100592, 0.92899408],\n",
       "       [1.        , 0.        ],\n",
       "       [0.03184713, 0.96815287],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.0617284 , 0.9382716 ],\n",
       "       [0.99465241, 0.00534759],\n",
       "       [0.92349727, 0.07650273],\n",
       "       [0.79329609, 0.20670391],\n",
       "       [0.55978261, 0.44021739],\n",
       "       [0.        , 1.        ],\n",
       "       [0.12359551, 0.87640449],\n",
       "       [1.        , 0.        ],\n",
       "       [0.97237569, 0.02762431],\n",
       "       [0.98324022, 0.01675978],\n",
       "       [1.        , 0.        ],\n",
       "       [0.01156069, 0.98843931],\n",
       "       [0.        , 1.        ],\n",
       "       [0.46511628, 0.53488372],\n",
       "       [0.83783784, 0.16216216],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.02051282, 0.97948718],\n",
       "       [0.        , 1.        ],\n",
       "       [0.96685083, 0.03314917],\n",
       "       [0.        , 1.        ],\n",
       "       [0.2173913 , 0.7826087 ],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.97311828, 0.02688172],\n",
       "       [0.87356322, 0.12643678],\n",
       "       [1.        , 0.        ],\n",
       "       [0.00561798, 0.99438202],\n",
       "       [0.06395349, 0.93604651],\n",
       "       [0.99418605, 0.00581395],\n",
       "       [0.03773585, 0.96226415],\n",
       "       [0.        , 1.        ],\n",
       "       [0.08571429, 0.91428571],\n",
       "       [1.        , 0.        ],\n",
       "       [0.84431138, 0.15568862],\n",
       "       [0.        , 1.        ],\n",
       "       [0.91326531, 0.08673469],\n",
       "       [0.98947368, 0.01052632],\n",
       "       [0.14124294, 0.85875706],\n",
       "       [0.21505376, 0.78494624],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.23039216, 0.76960784],\n",
       "       [0.95628415, 0.04371585],\n",
       "       [0.00561798, 0.99438202],\n",
       "       [1.        , 0.        ],\n",
       "       [0.97701149, 0.02298851],\n",
       "       [0.        , 1.        ],\n",
       "       [0.52197802, 0.47802198],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.08333333, 0.91666667],\n",
       "       [0.10362694, 0.89637306],\n",
       "       [0.99441341, 0.00558659],\n",
       "       [0.01075269, 0.98924731],\n",
       "       [1.        , 0.        ],\n",
       "       [0.41798942, 0.58201058],\n",
       "       [0.1027027 , 0.8972973 ],\n",
       "       [0.54166667, 0.45833333],\n",
       "       [0.63541667, 0.36458333],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.58598726, 0.41401274],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.18536585, 0.81463415],\n",
       "       [0.84357542, 0.15642458],\n",
       "       [0.04545455, 0.95454545],\n",
       "       [1.        , 0.        ],\n",
       "       [0.81111111, 0.18888889],\n",
       "       [0.        , 1.        ],\n",
       "       [0.00578035, 0.99421965],\n",
       "       [0.14204545, 0.85795455],\n",
       "       [0.01470588, 0.98529412],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.90285714, 0.09714286],\n",
       "       [0.17837838, 0.82162162],\n",
       "       [0.94350282, 0.05649718],\n",
       "       [0.01092896, 0.98907104],\n",
       "       [0.65898618, 0.34101382],\n",
       "       [0.07734807, 0.92265193],\n",
       "       [0.98295455, 0.01704545],\n",
       "       [0.79428571, 0.20571429],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.94117647, 0.05882353],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.32984293, 0.67015707],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.84210526, 0.15789474],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.77018634, 0.22981366],\n",
       "       [0.90810811, 0.09189189],\n",
       "       [1.        , 0.        ],\n",
       "       [0.75141243, 0.24858757],\n",
       "       [0.51207729, 0.48792271],\n",
       "       [0.        , 1.        ],\n",
       "       [0.90055249, 0.09944751],\n",
       "       [0.00552486, 0.99447514],\n",
       "       [1.        , 0.        ],\n",
       "       [0.9010989 , 0.0989011 ],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.76213592, 0.23786408],\n",
       "       [0.12432432, 0.87567568],\n",
       "       [0.53142857, 0.46857143],\n",
       "       [0.23529412, 0.76470588],\n",
       "       [0.        , 1.        ],\n",
       "       [0.91256831, 0.08743169],\n",
       "       [0.80412371, 0.19587629],\n",
       "       [0.01092896, 0.98907104],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.02150538, 0.97849462],\n",
       "       [0.95360825, 0.04639175],\n",
       "       [0.94578313, 0.05421687],\n",
       "       [1.        , 0.        ],\n",
       "       [0.52331606, 0.47668394],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.97777778, 0.02222222],\n",
       "       [0.0255102 , 0.9744898 ],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.97727273, 0.02272727],\n",
       "       [0.        , 1.        ],\n",
       "       [0.09248555, 0.90751445],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.99468085, 0.00531915],\n",
       "       [0.01069519, 0.98930481],\n",
       "       [1.        , 0.        ],\n",
       "       [0.09392265, 0.90607735],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.3964497 , 0.6035503 ],\n",
       "       [0.07142857, 0.92857143],\n",
       "       [0.24309392, 0.75690608],\n",
       "       [1.        , 0.        ],\n",
       "       [0.98429319, 0.01570681],\n",
       "       [0.22222222, 0.77777778],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.92307692, 0.07692308],\n",
       "       [0.34554974, 0.65445026],\n",
       "       [0.99487179, 0.00512821],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.02688172, 0.97311828],\n",
       "       [0.9895288 , 0.0104712 ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.01129944, 0.98870056],\n",
       "       [0.63101604, 0.36898396]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bag_clf.oob_decision_function_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Patches and Random Subspaces\n",
    "* BaggingClassifier **class supports sampling the features as well**\n",
    "* **Sampling is controlled by two hyperparameters:** max_features **and** bootstrap_features**\n",
    "* **Each predictor will be trained on a random subset of the input features**\n",
    "* **This technique is particularly useful when you are dealing with high dimensional inputs**\n",
    "* ***Random Patches* method: Sampling both trainning instances and features**\n",
    "* ***Random Subspaces method:* Keeping all trainning instances(by setting** bootstrap=False **and** max_samples=1.0 **) but sampling features.**\n",
    "* **Sampling features results in even more predictor diversity, trading a bit more bias for a lower variance**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Radom Forests\n",
    "**Random Forest is an ensemble of Decision Trees, generally trained via the bagging method(or sometimes pasting), typically with** max_samples **set to the size of the trainning set**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rnd_clf=RandomForestClassifier(n_estimators=500,max_leaf_nodes=16,n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(max_leaf_nodes=16, n_estimators=500, n_jobs=-1)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rnd_clf.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_rf=rnd_clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.912"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_test,y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **With a few exceptions, a** RandomForestClassifier **has all the hyperparameters of a** DecisionTreeCalssifier **(to control how trees are grown), plus all the hyperparameters of a** BaggingClassifier **to control the ensemble itself**\n",
    "* **Random Forest algorithm introduces extra randomness when grwoing tree; instead of serarching for the very best features when splitting a node, it searches for the best feature among a random subset of features**\n",
    "* **Results in greater diversity,trades higher bias for lower bariance, generally yielding an overall better model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Equivalent to the previous RandomForestClassifier\n",
    "bag_clf=BaggingClassifier(\n",
    "    DecisionTreeClassifier(max_features=\"auto\",max_leaf_nodes=16),\n",
    "    n_estimators=500,max_samples=1.0,bootstrap=True,n_jobs=-1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BaggingClassifier(base_estimator=DecisionTreeClassifier(max_features='auto',\n",
       "                                                        max_leaf_nodes=16),\n",
       "                  n_estimators=500, n_jobs=-1)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bag_clf.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_bc=bag_clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.912"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_test,y_pred_bc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extra-Trees\n",
    "* **When you are growing a tree in a Random Forest, at each node only a random subset of the features is considered for splitting.**\n",
    "* **This technique trades more bias for a lower variance.**\n",
    "* **It also maked Extra-Trees much faster to train than a regular Random Forests**\n",
    "* **You can create an Extra-Trees classifier using SK-Learn's** ExtraTreesClassifier, **its API is identical to the** RandomForestClassifier\n",
    "* **The** ExtraTreesRegressor **class has the same API as the** RandomForestRegressor\n",
    "* **It is hard to tell in adcance whether a** RandomForestClassifier **will perform better or worse than an** ExtraTreesClassifier. **Generally, the only way to know is to try both and compare them using cross-validation(tunning the hyperparameter using grid search)**\n",
    "\n",
    "### Feature Importance\n",
    "* **Another great quality of Random Forests is that they make it easy to measure the realtive importance of each feature**\n",
    "* **SK-Learn measures a feature's importance by looking at how much the tree nodes that use that feature reduce impurity on average(across all trees in the forest).**\n",
    "* **SK-Learn automatically computes feature importance for each feature after trainning, then it scales the results so that the sum of all importances is equal to 1**\n",
    "* **You can access the result using the** _feature_importance_ **variable**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris=load_iris()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnd_clf=RandomForestClassifier(n_estimators=500,n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(n_estimators=500, n_jobs=-1)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rnd_clf.fit(iris[\"data\"],iris['target'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sepal length (cm) 0.10358326325367606\n",
      "sepal width (cm) 0.022052524481100653\n",
      "petal length (cm) 0.4496100165010137\n",
      "petal width (cm) 0.4247541957642097\n"
     ]
    }
   ],
   "source": [
    "for name,score in zip(iris[\"feature_names\"],rnd_clf.feature_importances_):\n",
    "    print(name,score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Random Forests are very handy to get a quick understanding of what features actually matter, in particular if you need to perform feature selection**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Boosting\n",
    "**Refers to any Ensemble method that can combine several weak learners into a strong learner**<br>\n",
    "**The general idea of most boosting methods is to train predictors sequentially, each trying to correct its predecessors**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AdaBoost\n",
    "* **A new predictor correct its predeccessor's mistake by paying a bit more attention to the training instances that the predecessor underfitted**\n",
    "* **This resulted in new predictors focusing more on the hard cases**\n",
    "* **AdaBoost Algorithm**\n",
    "    * **When training an AdaBoost classifier, the algorithm first trains a base classifier and uses it to make predictions on the training set**\n",
    "    * **The algorithm then increases the relative weight of misclassified training instances**\n",
    "    * **Then it trains a second classifier, using the updated weights, and again makes predictions on the training set, updates the instance weights, and so on**\n",
    "* **Once all predictors are trained, the ensemble makes predictions very much like bagging or pasting, expcet that predictors have different weights depending on their overall accuaracy on the weighted trainning set**\n",
    "* **There is one importany drawaback to this sequential learning technique: it cannot be parallelized(or only partially), since each predictor can only be trained after the previous predictor has been trained and evaluated. It does not scale as well as bagging or pasting**\n",
    "* **SK-Learn uses a multiclass version of AdaBoost called SAMME. If the predictors can estimate class porbabilities(i.e., if they have a** predict_proba( ) **method), SK-Learn can use a variant of SAMME called *SAMMER.R*, which relies on class probabilities rather than predictiuons and generally performs better**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Following code trains an AdaBoost classifier based on *200 Decision Stumps* using SK-Learn** AdaBoostClassifier **class**\n",
    "* **A Decision Stump is a Decision Tree with** max_depth=1, **a tree composed of a single decision node plus two leaft nodes**\n",
    "* **This is the default base estimator for the** AdaBoostClassifier **class**\n",
    "* **If your AdaBoost ensemble is overfitting the trainning set, you can try reducing the number of estimators or more strongly regularizing the base estimator**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "ada_clf=AdaBoostClassifier(\n",
    "    DecisionTreeClassifier(max_depth=1),n_estimators=200,\n",
    "    algorithm=\"SAMME.R\",learning_rate=0.5\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Boosting\n",
    "* **Sequentially adding predictors to an ensemble, each one correcting its predictors**\n",
    "* **Instead of tweaking the instance weghts at every iteration like AdaBoost does, this method tries to fit the new predictor to the *residual errors* made by the previous predictor**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.random.seed(42)\n",
    "X = np.random.rand(100, 1) - 0.5\n",
    "y = 3*X[:, 0]**2 + 0.05 * np.random.randn(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_reg1=DecisionTreeRegressor(max_depth=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeRegressor(max_depth=2)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tree_reg1.fit(X,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Train a second** DecisionTreeRegressor **on the residual errors made by the first predictor**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "y2=y-tree_reg1.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_reg2=DecisionTreeRegressor(max_depth=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeRegressor(max_depth=2)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tree_reg2.fit(X,y2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Train a third regressor on the residual errors made by the second predictor**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "y3=y2-tree_reg2.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_reg3=DecisionTreeRegressor(max_depth=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeRegressor(max_depth=3)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tree_reg3.fit(X,y3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**We have an ensemble containing three trees. It can make predictions on new instance by simply adding up the predictions for all the trees**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_new = np.array([[0.8]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred=sum(tree.predict(X_new) for tree in [tree_reg1,tree_reg2,tree_reg3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.75026781])"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Ensemble's predictions gradually get better as trees are added to the ensemble**\n",
    "* **SK-Learn's** GradientBoostingRegressor **class. It has hyperparameters to control the growth of Decision Trees(e.g.,** max_depth, min_samples_leaf **), as well as hyperparamters to control the ensemble training, such as the number of trees(** n_estimatros)\n",
    "* **Shrinkage: The** learning_rate **parameter scales the contribution of each tree. If you set it to a low value, such as 0.1, you will need more trees in the ensemble to fit the trainning set, but the predicitions will usually generalize better**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbrt=GradientBoostingRegressor(max_depth=2,n_estimators=3,learning_rate=1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **In order to find the optimal number of trees, you can use early stopping.**\n",
    "* **A simply way to implement this is to use the** staged_predict( ) **method: it returns an iterator over the predictions made by the ensemble at each stage of trainning**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,X_val,y_train,y_val=train_test_split(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbrt=GradientBoostingRegressor(max_depth=2,n_estimators=120)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GradientBoostingRegressor(max_depth=2, n_estimators=120)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gbrt.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "errors=[mean_squared_error(y_val,y_pred)\n",
    "       for y_pred in gbrt.staged_predict(X_val)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_n_estimators=np.argmin(errors)+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbrt_best=GradientBoostingRegressor(max_depth=2,n_estimators=best_n_estimators)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GradientBoostingRegressor(max_depth=2, n_estimators=53)"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gbrt_best.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **It is also possible to implement early stopping by actually stopping training early**\n",
    "* **You can do so by setting** warm_start=True, **which makes SK-Learn keep existing trees when the** fit( ) **method is called, allowing incremental trainning**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Stops trainning when the validation error does not improve for five iterations in a row**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbrt=GradientBoostingRegressor(max_depth=2,warm_start=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_val_error=float(\"inf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "error_going_up=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "for n_estimators in range(1,120):\n",
    "    gbrt.n_estimators=n_estimators\n",
    "    gbrt.fit(X_train,y_train)\n",
    "    y_pred=gbrt.predict(X_val)\n",
    "    val_error=mean_squared_error(y_val,y_pred)\n",
    "    if val_eror<min_val_error:\n",
    "        min_val_error=val_error\n",
    "        error_going_up=0\n",
    "    else:\n",
    "        error_going_up+=1\n",
    "        if error_going_up==5:\n",
    "            break #early stopping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Stochastic Gradient Boosting:** GadientBoostingRegressor **also support a** subsample **hyperparameter, which specifies the fraction of trainning instance to be used for trainning each tree**\n",
    "* **This technique trades a higher bias for a lower variance. It also speeds up training considerably**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**An optimized implementation of Gradient Boosting is availiable in popular Python library XGBoost, whcih stands for Extreme Gradient Boosting.**\n",
    "* **The interface is similar to SK-Learn**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_reg=xgboost.XGBRegressor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,\n",
       "             importance_type='gain', interaction_constraints='',\n",
       "             learning_rate=0.300000012, max_delta_step=0, max_depth=6,\n",
       "             min_child_weight=1, missing=nan, monotone_constraints='()',\n",
       "             n_estimators=100, n_jobs=0, num_parallel_tree=1, random_state=0,\n",
       "             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
       "             tree_method='exact', validate_parameters=1, verbosity=None)"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgb_reg.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred=xgb_reg.predict(X_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**XGBoost cab automatically take care of early stopping**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\tvalidation_0-rmse:0.21083\n",
      "Will train until validation_0-rmse hasn't improved in 2 rounds.\n",
      "[1]\tvalidation_0-rmse:0.14897\n",
      "[2]\tvalidation_0-rmse:0.11373\n",
      "[3]\tvalidation_0-rmse:0.09167\n",
      "[4]\tvalidation_0-rmse:0.07759\n",
      "[5]\tvalidation_0-rmse:0.06909\n",
      "[6]\tvalidation_0-rmse:0.06445\n",
      "[7]\tvalidation_0-rmse:0.06278\n",
      "[8]\tvalidation_0-rmse:0.06152\n",
      "[9]\tvalidation_0-rmse:0.06114\n",
      "[10]\tvalidation_0-rmse:0.06077\n",
      "[11]\tvalidation_0-rmse:0.06146\n",
      "[12]\tvalidation_0-rmse:0.06154\n",
      "Stopping. Best iteration:\n",
      "[10]\tvalidation_0-rmse:0.06077\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,\n",
       "             importance_type='gain', interaction_constraints='',\n",
       "             learning_rate=0.300000012, max_delta_step=0, max_depth=6,\n",
       "             min_child_weight=1, missing=nan, monotone_constraints='()',\n",
       "             n_estimators=100, n_jobs=0, num_parallel_tree=1, random_state=0,\n",
       "             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
       "             tree_method='exact', validate_parameters=1, verbosity=None)"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgb_reg.fit(X_train,y_train,\n",
    "           eval_set=[(X_val,y_val)],early_stopping_rounds=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred=xgb_reg.predict(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.1885474 ,  0.5384966 , -0.01354021,  0.58057415,  0.1885474 ,\n",
       "        0.0152958 ,  0.67034435,  0.0152958 ,  0.12898493,  0.1885474 ,\n",
       "        0.0152958 ,  0.1643205 ,  0.5021704 ,  0.15496236,  0.28682178,\n",
       "        0.55297387,  0.47523323,  0.35735914,  0.68745124,  0.47523323,\n",
       "        0.57613474,  0.12898493,  0.33966637,  0.1885474 ,  0.1643205 ],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stacking\n",
    "* **Instead of using trivial functions to aggregate the predictions of all predictiors in an ensemble, why don't we train a model to perform aggregation**\n",
    "* **To train the blender, a common approach is to use a hold-out set**\n",
    "    1. **First, the trainning set is split into two subsets. The first subset is used to train the predictors in the first layer**\n",
    "    1. **Next, the first layer's predictors are used to make predictions on the second(held-out) set.**\n",
    "    1. **We can create a new training set using the predicted values, as input features, ans keeping the target values**\n",
    "    1. **The blender is trained on the new trainning set, so it learns to preduct the target valye, given the first layer's predictions** \n",
    "    1. **It is possible to train several different blenders this way, the trick is to split the training set into three subsets: the first one is used to train the first layer, the second one is used to create the training set used to train the second layer(using predictions made by the predictors of the first layer), and the third one is used to create the training set to train the third layer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

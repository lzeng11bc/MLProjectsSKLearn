{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 8. Dimensionality Reduction\n",
    "* ***Curse of dimensionality*: Many Machine Learning Porblems involve thousands or even millions of features for each training instance. Not only do all these features make training extremely slow, but they can also make it much harder to find a good solution**\n",
    "* **Reducing dimensionality does cause some information loss, so even though it will speed up training, it may make your system perform slightly worse**\n",
    "* **If training is too slow, you should first try to train your system with the original data before considering using dimensionality reduction**\n",
    "* **Dimensionality reduction is also extremely useful for data visualization.Reducing the number of dimensions down to two(or three) makes it possible to plot a condensed view of a high-dimenional training set on a graph and often gain some important insights by visually detecing patterns, such as clusters**\n",
    "\n",
    "## Curse of Dimensionality\n",
    "* **The more dimensions the training set has, the greater the risk of overfitting it**\n",
    "* **One solution to the curse of the dimensionaluty could be to increase the size of the training set to reach a sufficient density of training instaces**\n",
    "* **In practice, the number of training instances required to reach a given density grows exponentially with the number of dimensions**\n",
    "\n",
    "## Main Approaches for Dimensionality Reduction\n",
    "\n",
    "### Projection\n",
    "* **In most real-world problems, training instances are *spread* out uniformly across all dimensions. As a result, all training instances lie within(or close to) a much lower-dimensional *subspace* of the high-dimensional space**\n",
    "\n",
    "### Manifold Learning\n",
    "* **A *d*-dimensional manifolf is a part of an *n*-dimensional space(where *d\\<n*) that locallay resembles a d-dimensional hyperplane**\n",
    "* ***Manifold Learning*: Many dimensionality reduction algorithms work by modeling the manifold on which the training instances lie**\n",
    "    * **Relies on the *manifold assumption(manifold hypothesis):* Holds that most real-world high dimensional datasets lie close to a much lower-dimensional manifold**\n",
    "    * **Assumption2: the task at hand will be simpler if expressed in the lower-dimensional space of the manifold(May not always hold)**\n",
    "\n",
    "**Reducing the dimensionality of your training set before training a model will usually speed up training, but it may not always lead to a better of simpler solution; it all depends on the dataset.**\n",
    "\n",
    "## PCA\n",
    "### Preserving the Variance\n",
    "* **It seems reasonable to select the axis that preserves the maximum amount of variance, as it will most likely lose less information than the other projections**\n",
    "* **Another way to justify this choice is that it is the axis that minimizes the mean squared distance between the original dataset and its projection onto that axis**\n",
    "\n",
    "### Principal Components\n",
    "* **PCA identifies the axis that accounts for the largest amount of varaince in the training set**\n",
    "* **It also finds a second axis, orthogonal to the first one, that accounts for the largest amount of remaining variance**\n",
    "* **The ith axis is called the ith *principal component*(PC) of the data**\n",
    "* **For each principal component, PCA finds a zero-centered unit vector pointing in the direction of the PC. Since two opposing unit vectors lie on the same axis, the direction of the unit vectors returned by PCA is not stable**\n",
    "* **We can use a standard matrix factorization technique called *Singular Value Decomposition*(SVD) that can decompose the training set matrix *X* into the matrix multiplication of three matrices.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Build a 3D model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(4)\n",
    "m = 60\n",
    "w1, w2 = 0.1, 0.3\n",
    "noise = 0.1\n",
    "\n",
    "angles = np.random.rand(m) * 3 * np.pi / 2 - 0.5\n",
    "X = np.empty((m, 3))\n",
    "X[:, 0] = np.cos(angles) + np.sin(angles)/2 + noise * np.random.randn(m) / 2\n",
    "X[:, 1] = np.sin(angles) * 0.7 + noise * np.random.randn(m) / 2\n",
    "X[:, 2] = X[:, 0] * w1 + X[:, 1] * w2 + noise * np.random.randn(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_centered=X-X.mean(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "U,s,Vt=np.linalg.svd(X_centered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "c1=Vt.T[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "c2=Vt.T[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.93636116, 0.29854881, 0.18465208])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.34027485,  0.90119108,  0.2684542 ])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**PCA assumes that the dataset is centered around the origin. If you use SK-Learn's PCA, SK-Learn will take care of centering the data for you**\n",
    "\n",
    "### Projecting Down to d Dimensions\n",
    "* **Once you have identifies all the principal components, you can reduce the dimensionality of the dataset down to *d* dimensions by projecting it onto the hyperplane defined by the first *d* components**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Projects the trainning set onto the plane defined by the first two principal components**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "W2=Vt.T[:,:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "X2D=X_centered.dot(W2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.26203346, -0.42067648],\n",
       "       [ 0.08001485,  0.35272239],\n",
       "       [-1.17545763, -0.36085729],\n",
       "       [-0.89305601,  0.30862856],\n",
       "       [-0.73016287,  0.25404049],\n",
       "       [ 1.10436914, -0.20204953],\n",
       "       [-1.27265808, -0.46781247],\n",
       "       [ 0.44933007, -0.67736663],\n",
       "       [ 1.09356195,  0.04467792],\n",
       "       [ 0.66177325,  0.28651264]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X2D[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Scikit-Learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca=PCA(n_components=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "X2D=pca.fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explained Variance Ratio\n",
    "* **The ratio indicates the proportion of the dataset's variance taht lies along each principal component**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.84248607, 0.14631839])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pca.explained_variance_ratio_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choosing the Right Number of Dimensions\n",
    "* **Instead of arbitrarily choosing the number of dimensions to reduce down to, it is simpler to choose the number of dimensions that add up to a sufficiently large portion of the variance**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#computes the minimum number of dimensions required to preserve 95% of the training set's variance\n",
    "pca=PCA()\n",
    "pca.fit(X)\n",
    "cumsum=np.cumsum(pca.explained_variance_ratio_)\n",
    "d=np.argmax(cumsum>=0.95)+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca=PCA(n_components=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PCA(n_components=2)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pca.fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "X2D=pca.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Another option is to plot the explained variance as a function of the number of dimensionsï¼ˆsimply plot cumsum)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(cumsum)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA for Compression\n",
    "* **After dimensionality reduction, the training set takes up much less space**\n",
    "* **It is possible to decompress the reduced dataset back to the original dimension by appying the inverse trainsformation of the PCA projection**\n",
    "* **The mean squred distance between the original data and teh reconstructed data is callled *reconstruction error***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca=PCA(n_components=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_reduced=pca.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_recovered=pca.inverse_transform(X_reduced)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.04769668680511293"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_squared_error(X,X_recovered)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Randomized PCA\n",
    "* **If you set the** svd_solver **hyperparameter to** \"randomized\", **SK-Learn uses a stochastic algorithm called** *Randomized PCA* **that quickly finds an approximation of the first** *d* **principal compoents, it is faster than the full SVD when** *d* **is much smaller than *n***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnd_pca=PCA(n_components=1,svd_solver=\"randomized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_reduced=rnd_pca.fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Incremental PCA\n",
    "* **Allows you to split the training set into mini-batches and feed and ICPA algorithm one mini-batch at a time**\n",
    "* **You must call the** partial_fit( ) **method with each mini-batch, rather than the** fit( ) **method with the whole training set**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import IncrementalPCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_batches=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "inc_pca=IncrementalPCA(n_components=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "for X_batch in np.array_split(X,n_batches):\n",
    "    inc_pca.partial_fit(X_batch)\n",
    "    \n",
    "X_reduced=inc_pca.transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.26203346, -0.42067648],\n",
       "       [ 0.08001485,  0.35272239],\n",
       "       [-1.17545763, -0.36085729],\n",
       "       [-0.89305601,  0.30862856],\n",
       "       [-0.73016287,  0.25404049],\n",
       "       [ 1.10436914, -0.20204953],\n",
       "       [-1.27265808, -0.46781247],\n",
       "       [ 0.44933007, -0.67736663],\n",
       "       [ 1.09356195,  0.04467792],\n",
       "       [ 0.66177325,  0.28651264]])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_reduced[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Alternatively, you can use NumPy's** memmap **class, which allows you to manipulate a large array stored in a binary file on disk if it were entirely in memory; the class loads only the data it needs in memory, when it needs it**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_mm=np.memmap(filename,dtype=\"float32\",mode=\"readonly\",shape=(m,n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=m//n_batches\n",
    "inc_pca=IncrementalPCA(n_components=154,batch_size=batch_size)\n",
    "inc_pca.fit(X_mm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kernel PCA\n",
    "* **Makes it possible to perform complex nonlinear projections for dimensionality reduction**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import KernelPCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "rbf_pca=KernelPCA(n_components=2,kernel=\"rbf\",gamma=0.04)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_reduced=rbf_pca.fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Selecting a Kernel and Tuning Hyperparameters\n",
    "* **Following code creates a two-step pipeline**\n",
    "* **First, reducing the dimensionality to two dimensions using kPCA, then applying Logistic Regression for classification**\n",
    "* **Then it uses** GridSearchCV **to fidn the best kernel and** gamma **value for kPCA in order to get the best classification accuracy at the end of the pipeline**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf=Pipeline([\n",
    "    (\"kpca\",KernelPCA(n_components=2)),\n",
    "    (\"Log_reg\",LogisticRegression())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid=[{\n",
    "    \"kpca_gamma\":np.linspace(0.03,0.05,10),\n",
    "    \"kpca_kernel\":[\"rbf\",\"sigmoid\"]\n",
    "}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search=GridSearchCV(clf,param_grid,cv=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search.fit(X,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The best kernel and hyperparameters are then availiable through the** best_params_ **variable**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(grid_search.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Another approach is to select the kernel and hyperparameters that yield the lowest reconstruction error**\n",
    "   * **SK  Learn trains a supervised regression model, with the projected instance as the training set and the original instances as the targets**\n",
    "   * **SK-Learn will do this automatically if you set** fit_inverse_transform=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "rbf_pca=KernelPCA(n_components=2,kernel=\"rbf\",gamma=0.0433,\n",
    "                 fit_inverse_transform=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_reduced=rbf_pca.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_preimage=rbf_pca.inverse_transform(X_reduced)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.408975126514001e-31"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "mean_squared_error(X,X_preimage)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LLE\n",
    "* ***Locally Linear Embedding(LLE)*: A manifold learning technique**\n",
    "* **LLE works by first measuring how much each training instance linearly relates to its closet neighbors(c.n.), and then looking for a low dimensional representation of the training set where these local relationships are best preserved**\n",
    "* **This approach makes it particularly good at unrolling twisted manifolds, especially when there is not too much noise**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_swiss_roll\n",
    "X, t = make_swiss_roll(n_samples=1000, noise=0.2, random_state=41)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import LocallyLinearEmbedding\n",
    "lle=LocallyLinearEmbedding(n_components=2,n_neighbors=10)\n",
    "X_reduced=lle.fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other Dimensionality Reduction Techniques\n",
    "* **Random Projections**:\n",
    "    * **Projects the data to a lower-dimensional space using a random linear projection**\n",
    "    * **The quality of the dimensionality reduction depends on the number of instances and the target dimensionality, but not the initial dimensionality**\n",
    "    * **In** Sklearn.random_projections\n",
    "* **Multdimensional Scaling(MDS):**\n",
    "    * **Reduces dimensionality while trying to preserve the distances between the instances**\n",
    "* **Isomap:**\n",
    "    * **Create a graphy by connecting each instance to its nearest neighbors, then reduces dimensionality while trying to preserve the *geodesic distances* between the instances**\n",
    "* **t-Distributed Stochastic Neighbor Embedding(t-SNE):**\n",
    "    * **Reduces dimensionality while trying to keep similat instances close and dissimilar instances aprat**\n",
    "    * **Mostly used for visulisation**\n",
    "* **Linear Discriminat Analysis**\n",
    "    * **Is a classification algorithm, but during training learns the most discriminative axes between the classes, and these axes can then be used to define a hyperplane onto which to project the data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "On branch master\n",
      "Your branch is up to date with 'origin/master'.\n",
      "\n",
      "Changes not staged for commit:\n",
      "  (use \"git add <file>...\" to update what will be committed)\n",
      "  (use \"git restore <file>...\" to discard changes in working directory)\n",
      "\tmodified:   ch7_ensemble_learning_and_random_forest.ipynb\n",
      "\n",
      "Untracked files:\n",
      "  (use \"git add <file>...\" to include in what will be committed)\n",
      "\t.DS_Store\n",
      "\t.ipynb_checkpoints/\n",
      "\tChapter  4 Training Model.docx\n",
      "\tChapter 5 Support Vector Machines.docx\n",
      "\tChapter 6 Decision Trees.docx\n",
      "\tChapter 7 Ensemble Learning and Random Forest.docx\n",
      "\tbc.ipynb\n",
      "\tbc1.csv\n",
      "\tch8_dimensionality_reduction.ipynb\n",
      "\timages/\n",
      "\tiris\n",
      "\n",
      "no changes added to commit (use \"git add\" and/or \"git commit -a\")\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "git status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "git add \"ch8_dimensionality_reduction.ipynb\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[master ca952ed] Add dimensionality reduction\n",
      " 1 file changed, 1028 insertions(+)\n",
      " create mode 100644 ch8_dimensionality_reduction.ipynb\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "git commit -m\"Add dimensionality reduction\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "To https://github.com/lzeng11bc/MLProjectsSKLearn.git\n",
      "   8988f93..ca952ed  master -> master\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "git push"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.3 64-bit",
   "language": "python",
   "name": "python38364bit719a10165d2e45edb91e9b20e26e67b0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
